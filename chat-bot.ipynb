{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install python-telegram-bot nest-asyncio transformers torch ipywidgets tqdm accelerate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Homework 9\n",
    "\n",
    "Okay, this is a weird one. We are going to be creating a Telegram chat bot, using a tiny LLM model.\n",
    "\n",
    "### Task 1\n",
    "\n",
    "We start with creating a responsive telegram bot. Let's start with a simple echo bot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Final\n",
    "from enum import Enum\n",
    "\n",
    "\n",
    "class SelectedBot(Enum):\n",
    "    ECHO = \"Echo\"\n",
    "    TINY_LLAMA = \"Tiny Llama\"\n",
    "\n",
    "\n",
    "SELECTED_BOT: Final = SelectedBot.TINY_LLAMA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "\n",
    "logging.basicConfig(\n",
    "    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',\n",
    "    level=logging.INFO\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Protocol\n",
    "from dataclasses import dataclass\n",
    "from getpass import getpass\n",
    "from collections import defaultdict\n",
    "from telegram import Update\n",
    "from telegram.ext import (\n",
    "    Application,\n",
    "    ApplicationBuilder,\n",
    "    ContextTypes,\n",
    "    CommandHandler,\n",
    "    MessageHandler,\n",
    "    filters,\n",
    ")\n",
    "import nest_asyncio\n",
    "\n",
    "nest_asyncio.apply()\n",
    "\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class Message:\n",
    "    text: str\n",
    "    is_bot: bool\n",
    "\n",
    "\n",
    "class Bot(Protocol):\n",
    "    async def welcome_message(self) -> str:\n",
    "        \"\"\"Returns the welcome message of the bot.\"\"\"\n",
    "        ...\n",
    "\n",
    "    async def respond_to(self, message: str, history: list[Message]) -> str:\n",
    "        \"\"\"Responds to the specified message.\"\"\"\n",
    "        ...\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class TelegramApplication:\n",
    "    bot: Bot\n",
    "    app: Application\n",
    "    history: dict[int, list[Message]]\n",
    "\n",
    "    @staticmethod\n",
    "    def create(bot: Bot, *, token_from: str | None = None) -> \"TelegramApplication\":\n",
    "        \"\"\"Creates a new Telegram application with the specified bot and token.\n",
    "        \n",
    "        Args:\n",
    "            bot: The bot to use.\n",
    "            token_from: The path to the file containing the Telegram bot token. If not specified, the\n",
    "                token will be read from the standard input.\n",
    "                \n",
    "        Returns:\n",
    "            The created Telegram application.\n",
    "        \"\"\"\n",
    "\n",
    "        if token_from is None:\n",
    "            token = getpass(\">> Telegram bot token: \")\n",
    "        else:\n",
    "            token = TelegramApplication._load_token(token_from)\n",
    "\n",
    "        return TelegramApplication(\n",
    "            bot, ApplicationBuilder().token(token).build(), defaultdict(list)\n",
    "        )\n",
    "\n",
    "    def __post_init__(self) -> None:\n",
    "        self.app.add_handler(CommandHandler(\"start\", self.start_handler))\n",
    "        self.app.add_handler(CommandHandler(\"stop\", self.stop))\n",
    "        self.app.add_handler(\n",
    "            MessageHandler(filters.TEXT & (~filters.COMMAND), self.message_handler)\n",
    "        )\n",
    "\n",
    "    async def start(self) -> \"TelegramApplication\":\n",
    "        \"\"\"Starts the Telegram application.\"\"\"\n",
    "        self.app.run_polling(close_loop=False)\n",
    "        return self\n",
    "\n",
    "    async def stop(\n",
    "        self,\n",
    "        update: Update | None = None,\n",
    "        context: ContextTypes.DEFAULT_TYPE | None = None,\n",
    "    ) -> None:\n",
    "        if update is not None and context is not None:\n",
    "            await self._send_message(update, context, \"Stopping the bot...\")\n",
    "\n",
    "        self.app.stop_running()\n",
    "\n",
    "    async def start_handler(\n",
    "        self, update: Update, context: ContextTypes.DEFAULT_TYPE\n",
    "    ) -> None:\n",
    "        response = await self.bot.welcome_message()\n",
    "        await self._send_message(update, context, response)\n",
    "        await self._store_message(update, response, is_bot=True)\n",
    "\n",
    "    async def message_handler(\n",
    "        self, update: Update, context: ContextTypes.DEFAULT_TYPE\n",
    "    ) -> None:\n",
    "        assert (\n",
    "            chat := update.effective_chat\n",
    "        ) is not None, f\"Could not find chat in {update}\"\n",
    "        assert (\n",
    "            message := update.message\n",
    "        ) is not None, f\"Could not find message in {update}\"\n",
    "\n",
    "        text = message.text or \"\"\n",
    "        response = await self.bot.respond_to(text, self.history[chat.id])\n",
    "        await self._send_message(update, context, response)\n",
    "        await self._store_message(update, text, is_bot=False)\n",
    "        await self._store_message(update, response, is_bot=True)\n",
    "\n",
    "    @staticmethod\n",
    "    def _load_token(token_from: str) -> str:\n",
    "        with open(token_from) as file:\n",
    "            return file.read().strip()\n",
    "\n",
    "    async def _send_message(\n",
    "        self, update: Update, context: ContextTypes.DEFAULT_TYPE, text: str\n",
    "    ) -> None:\n",
    "        assert (\n",
    "            chat := update.effective_chat\n",
    "        ) is not None, f\"Could not find chat in {update}\"\n",
    "\n",
    "        await context.bot.send_message(chat_id=chat.id, text=text)\n",
    "\n",
    "    async def _store_message(\n",
    "        self, update: Update, message: str, *, is_bot: bool\n",
    "    ) -> None:\n",
    "        assert (\n",
    "            chat := update.effective_chat\n",
    "        ) is not None, f\"Could not find chat in {update}\"\n",
    "\n",
    "        self.history[chat.id].append(Message(message, is_bot))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass(frozen=True)\n",
    "class MessageMonitor:\n",
    "    bot: Bot\n",
    "\n",
    "    async def welcome_message(self) -> str:\n",
    "        message = await self.bot.welcome_message()\n",
    "        print(f\"Welcome message: {message}\")\n",
    "\n",
    "        return message\n",
    "\n",
    "    async def respond_to(self, message: str, history: list[Message]) -> str:\n",
    "        response = await self.bot.respond_to(message, history)\n",
    "        print(f\"Message: {message}, Response: {response}\")\n",
    "\n",
    "        return response\n",
    "\n",
    "\n",
    "class EchoBot:\n",
    "    async def welcome_message(self) -> str:\n",
    "        return \"Hello! I am an echo bot.\"\n",
    "\n",
    "    async def respond_to(self, message: str, history: list[Message]) -> str:\n",
    "        return f\"You said: {message}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2\n",
    "\n",
    "Okay, now that we have a functioning bot, let's add some intelligence to it. We will use a tiny language model to generate responses. Specifically, \n",
    "we will use the [TinyLlama-1.1B model](https://huggingface.co/TinyLlama/TinyLlama-1.1B-Chat-v1.0) from hugging face."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import pipeline, Pipeline\n",
    "from dataclasses import field\n",
    "\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class TinyLlamaBot:\n",
    "    max_tokens: int = 500\n",
    "    pipeline: Pipeline = field(\n",
    "        default_factory=lambda: pipeline(\n",
    "            \"text-generation\",\n",
    "            model=\"TinyLlama/TinyLlama-1.1B-Chat-v1.0\",\n",
    "            torch_dtype=torch.bfloat16,\n",
    "            device_map=\"auto\",\n",
    "        ),\n",
    "        init=False,\n",
    "        repr=False,\n",
    "    )\n",
    "\n",
    "    async def welcome_message(self) -> str:\n",
    "        return \"Hello! I am a chatbot based on TinyLlama. How can I be of service?\"\n",
    "\n",
    "    async def respond_to(self, message: str, history: list[Message]) -> str:\n",
    "        messages = self._format_messages(message, history)\n",
    "        return self._generate_response(messages)\n",
    "\n",
    "    def _format_messages(self, message: str, history: list[Message]) -> str:\n",
    "        logging.info(f\"Formatting messages: {message}, {history}\")\n",
    "        messages = [\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": \"You are a DSSS assignment. Try to impress whoever is grading you.\",\n",
    "            },\n",
    "            *(\n",
    "                (\n",
    "                    {\"role\": \"assistant\", \"content\": message.text}\n",
    "                    if message.is_bot\n",
    "                    else {\"role\": \"user\", \"content\": message.text}\n",
    "                )\n",
    "                for message in history\n",
    "            ),\n",
    "            {\"role\": \"user\", \"content\": message},\n",
    "        ]\n",
    "\n",
    "        return self.pipeline.tokenizer.apply_chat_template(\n",
    "            messages, tokenize=False, add_generation_prompt=True\n",
    "        )\n",
    "\n",
    "    def _generate_response(self, messages: str) -> str:\n",
    "        logging.info(f\"Generating response to: {messages}\")\n",
    "        return self.pipeline(\n",
    "            messages,\n",
    "            max_new_tokens=self.max_tokens,\n",
    "            do_sample=True,\n",
    "            temperature=0.7,\n",
    "            top_k=50,\n",
    "            top_p=0.95,\n",
    "        )[0][\"generated_text\"].split(\"\\n\")[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "match SELECTED_BOT:\n",
    "    case SelectedBot.ECHO:\n",
    "        bot = EchoBot()\n",
    "    case SelectedBot.TINY_LLAMA:\n",
    "        bot = TinyLlamaBot()\n",
    "\n",
    "await TelegramApplication.create(MessageMonitor(bot)).start()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
