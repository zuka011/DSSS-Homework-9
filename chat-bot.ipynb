{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution ~lotly (d:\\Projects\\Python\\dsss\\.venv\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~lotly (d:\\Projects\\Python\\dsss\\.venv\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~lotly (d:\\Projects\\Python\\dsss\\.venv\\Lib\\site-packages)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: python-telegram-bot in d:\\projects\\python\\dsss\\.venv\\lib\\site-packages (21.10)\n",
      "Requirement already satisfied: nest-asyncio in d:\\projects\\python\\dsss\\.venv\\lib\\site-packages (1.6.0)\n",
      "Requirement already satisfied: transformers in d:\\projects\\python\\dsss\\.venv\\lib\\site-packages (4.48.1)\n",
      "Requirement already satisfied: torch in d:\\projects\\python\\dsss\\.venv\\lib\\site-packages (2.5.1)\n",
      "Requirement already satisfied: ipywidgets in d:\\projects\\python\\dsss\\.venv\\lib\\site-packages (8.1.5)\n",
      "Requirement already satisfied: tqdm in d:\\projects\\python\\dsss\\.venv\\lib\\site-packages (4.67.1)\n",
      "Requirement already satisfied: accelerate in d:\\projects\\python\\dsss\\.venv\\lib\\site-packages (1.3.0)\n",
      "Requirement already satisfied: httpx~=0.27 in d:\\projects\\python\\dsss\\.venv\\lib\\site-packages (from python-telegram-bot) (0.27.2)\n",
      "Requirement already satisfied: filelock in d:\\projects\\python\\dsss\\.venv\\lib\\site-packages (from transformers) (3.17.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.24.0 in d:\\projects\\python\\dsss\\.venv\\lib\\site-packages (from transformers) (0.27.1)\n",
      "Requirement already satisfied: numpy>=1.17 in d:\\projects\\python\\dsss\\.venv\\lib\\site-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in d:\\projects\\python\\dsss\\.venv\\lib\\site-packages (from transformers) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in d:\\projects\\python\\dsss\\.venv\\lib\\site-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in d:\\projects\\python\\dsss\\.venv\\lib\\site-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: requests in d:\\projects\\python\\dsss\\.venv\\lib\\site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in d:\\projects\\python\\dsss\\.venv\\lib\\site-packages (from transformers) (0.21.0)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in d:\\projects\\python\\dsss\\.venv\\lib\\site-packages (from transformers) (0.5.2)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in d:\\projects\\python\\dsss\\.venv\\lib\\site-packages (from torch) (4.12.2)\n",
      "Requirement already satisfied: networkx in d:\\projects\\python\\dsss\\.venv\\lib\\site-packages (from torch) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in d:\\projects\\python\\dsss\\.venv\\lib\\site-packages (from torch) (3.1.4)\n",
      "Requirement already satisfied: fsspec in d:\\projects\\python\\dsss\\.venv\\lib\\site-packages (from torch) (2024.12.0)\n",
      "Requirement already satisfied: setuptools in d:\\projects\\python\\dsss\\.venv\\lib\\site-packages (from torch) (75.2.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in d:\\projects\\python\\dsss\\.venv\\lib\\site-packages (from torch) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in d:\\projects\\python\\dsss\\.venv\\lib\\site-packages (from sympy==1.13.1->torch) (1.3.0)\n",
      "Requirement already satisfied: comm>=0.1.3 in d:\\projects\\python\\dsss\\.venv\\lib\\site-packages (from ipywidgets) (0.2.2)\n",
      "Requirement already satisfied: ipython>=6.1.0 in d:\\projects\\python\\dsss\\.venv\\lib\\site-packages (from ipywidgets) (8.29.0)\n",
      "Requirement already satisfied: traitlets>=4.3.1 in d:\\projects\\python\\dsss\\.venv\\lib\\site-packages (from ipywidgets) (5.14.3)\n",
      "Requirement already satisfied: widgetsnbextension~=4.0.12 in d:\\projects\\python\\dsss\\.venv\\lib\\site-packages (from ipywidgets) (4.0.13)\n",
      "Requirement already satisfied: jupyterlab-widgets~=3.0.12 in d:\\projects\\python\\dsss\\.venv\\lib\\site-packages (from ipywidgets) (3.0.13)\n",
      "Requirement already satisfied: colorama in d:\\projects\\python\\dsss\\.venv\\lib\\site-packages (from tqdm) (0.4.6)\n",
      "Requirement already satisfied: psutil in d:\\projects\\python\\dsss\\.venv\\lib\\site-packages (from accelerate) (6.1.0)\n",
      "Requirement already satisfied: anyio in d:\\projects\\python\\dsss\\.venv\\lib\\site-packages (from httpx~=0.27->python-telegram-bot) (4.6.2.post1)\n",
      "Requirement already satisfied: certifi in d:\\projects\\python\\dsss\\.venv\\lib\\site-packages (from httpx~=0.27->python-telegram-bot) (2024.8.30)\n",
      "Requirement already satisfied: httpcore==1.* in d:\\projects\\python\\dsss\\.venv\\lib\\site-packages (from httpx~=0.27->python-telegram-bot) (1.0.6)\n",
      "Requirement already satisfied: idna in d:\\projects\\python\\dsss\\.venv\\lib\\site-packages (from httpx~=0.27->python-telegram-bot) (3.10)\n",
      "Requirement already satisfied: sniffio in d:\\projects\\python\\dsss\\.venv\\lib\\site-packages (from httpx~=0.27->python-telegram-bot) (1.3.1)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in d:\\projects\\python\\dsss\\.venv\\lib\\site-packages (from httpcore==1.*->httpx~=0.27->python-telegram-bot) (0.14.0)\n",
      "Requirement already satisfied: decorator in d:\\projects\\python\\dsss\\.venv\\lib\\site-packages (from ipython>=6.1.0->ipywidgets) (5.1.1)\n",
      "Requirement already satisfied: jedi>=0.16 in d:\\projects\\python\\dsss\\.venv\\lib\\site-packages (from ipython>=6.1.0->ipywidgets) (0.19.1)\n",
      "Requirement already satisfied: matplotlib-inline in d:\\projects\\python\\dsss\\.venv\\lib\\site-packages (from ipython>=6.1.0->ipywidgets) (0.1.7)\n",
      "Requirement already satisfied: prompt-toolkit<3.1.0,>=3.0.41 in d:\\projects\\python\\dsss\\.venv\\lib\\site-packages (from ipython>=6.1.0->ipywidgets) (3.0.48)\n",
      "Requirement already satisfied: pygments>=2.4.0 in d:\\projects\\python\\dsss\\.venv\\lib\\site-packages (from ipython>=6.1.0->ipywidgets) (2.18.0)\n",
      "Requirement already satisfied: stack-data in d:\\projects\\python\\dsss\\.venv\\lib\\site-packages (from ipython>=6.1.0->ipywidgets) (0.6.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in d:\\projects\\python\\dsss\\.venv\\lib\\site-packages (from jinja2->torch) (3.0.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in d:\\projects\\python\\dsss\\.venv\\lib\\site-packages (from requests->transformers) (3.4.0)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in d:\\projects\\python\\dsss\\.venv\\lib\\site-packages (from requests->transformers) (2.2.3)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.3 in d:\\projects\\python\\dsss\\.venv\\lib\\site-packages (from jedi>=0.16->ipython>=6.1.0->ipywidgets) (0.8.4)\n",
      "Requirement already satisfied: wcwidth in d:\\projects\\python\\dsss\\.venv\\lib\\site-packages (from prompt-toolkit<3.1.0,>=3.0.41->ipython>=6.1.0->ipywidgets) (0.2.13)\n",
      "Requirement already satisfied: executing>=1.2.0 in d:\\projects\\python\\dsss\\.venv\\lib\\site-packages (from stack-data->ipython>=6.1.0->ipywidgets) (2.1.0)\n",
      "Requirement already satisfied: asttokens>=2.1.0 in d:\\projects\\python\\dsss\\.venv\\lib\\site-packages (from stack-data->ipython>=6.1.0->ipywidgets) (2.4.1)\n",
      "Requirement already satisfied: pure-eval in d:\\projects\\python\\dsss\\.venv\\lib\\site-packages (from stack-data->ipython>=6.1.0->ipywidgets) (0.2.3)\n",
      "Requirement already satisfied: six>=1.12.0 in d:\\projects\\python\\dsss\\.venv\\lib\\site-packages (from asttokens>=2.1.0->stack-data->ipython>=6.1.0->ipywidgets) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "%pip install python-telegram-bot nest-asyncio transformers torch ipywidgets tqdm accelerate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Homework 9\n",
    "\n",
    "Okay, this is a weird one. We are going to be creating a Telegram chat bot, using a tiny LLM model.\n",
    "\n",
    "### Task 1\n",
    "\n",
    "We start with creating a responsive telegram bot. Let's start with a simple echo bot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Final\n",
    "from enum import Enum\n",
    "\n",
    "\n",
    "class SelectedBot(Enum):\n",
    "    ECHO = \"Echo\"\n",
    "    TINY_LLAMA = \"Tiny Llama\"\n",
    "\n",
    "\n",
    "SELECTED_BOT: Final = SelectedBot.TINY_LLAMA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "\n",
    "logging.basicConfig(\n",
    "    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',\n",
    "    level=logging.INFO\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Protocol\n",
    "from dataclasses import dataclass\n",
    "from collections import defaultdict\n",
    "from telegram import Update\n",
    "from telegram.ext import (\n",
    "    Application,\n",
    "    ApplicationBuilder,\n",
    "    ContextTypes,\n",
    "    CommandHandler,\n",
    "    MessageHandler,\n",
    "    filters,\n",
    ")\n",
    "import nest_asyncio\n",
    "\n",
    "nest_asyncio.apply()\n",
    "\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class Message:\n",
    "    text: str\n",
    "    is_bot: bool\n",
    "\n",
    "\n",
    "class Bot(Protocol):\n",
    "    async def welcome_message(self) -> str:\n",
    "        \"\"\"Returns the welcome message of the bot.\"\"\"\n",
    "        ...\n",
    "\n",
    "    async def respond_to(self, message: str, history: list[Message]) -> str:\n",
    "        \"\"\"Responds to the specified message.\"\"\"\n",
    "        ...\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class TelegramApplication:\n",
    "    bot: Bot\n",
    "    app: Application\n",
    "    history: dict[int, list[Message]]\n",
    "\n",
    "    @staticmethod\n",
    "    def create(bot: Bot, *, token_from: str | None = None) -> \"TelegramApplication\":\n",
    "        \"\"\"Creates a new Telegram application with the specified bot and token.\n",
    "        \n",
    "        Args:\n",
    "            bot: The bot to use.\n",
    "            token_from: The path to the file containing the Telegram bot token. If not specified, the\n",
    "                token will be read from the standard input.\n",
    "                \n",
    "        Returns:\n",
    "            The created Telegram application.\n",
    "        \"\"\"\n",
    "\n",
    "        if token_from is None:\n",
    "            token = input(\">> Telegram bot token: \")\n",
    "        else:\n",
    "            token = TelegramApplication._load_token(token_from)\n",
    "\n",
    "        return TelegramApplication(\n",
    "            bot, ApplicationBuilder().token(token).build(), defaultdict(list)\n",
    "        )\n",
    "\n",
    "    def __post_init__(self) -> None:\n",
    "        self.app.add_handler(CommandHandler(\"start\", self.start_handler))\n",
    "        self.app.add_handler(CommandHandler(\"stop\", self.stop))\n",
    "        self.app.add_handler(\n",
    "            MessageHandler(filters.TEXT & (~filters.COMMAND), self.message_handler)\n",
    "        )\n",
    "\n",
    "    async def start(self) -> \"TelegramApplication\":\n",
    "        \"\"\"Starts the Telegram application.\"\"\"\n",
    "        self.app.run_polling(close_loop=False)\n",
    "        return self\n",
    "\n",
    "    async def stop(\n",
    "        self,\n",
    "        update: Update | None = None,\n",
    "        context: ContextTypes.DEFAULT_TYPE | None = None,\n",
    "    ) -> None:\n",
    "        if update is not None and context is not None:\n",
    "            await self._send_message(update, context, \"Stopping the bot...\")\n",
    "\n",
    "        self.app.stop_running()\n",
    "\n",
    "    async def start_handler(\n",
    "        self, update: Update, context: ContextTypes.DEFAULT_TYPE\n",
    "    ) -> None:\n",
    "        response = await self.bot.welcome_message()\n",
    "        await self._send_message(update, context, response)\n",
    "        await self._store_message(update, response, is_bot=True)\n",
    "\n",
    "    async def message_handler(\n",
    "        self, update: Update, context: ContextTypes.DEFAULT_TYPE\n",
    "    ) -> None:\n",
    "        assert (\n",
    "            chat := update.effective_chat\n",
    "        ) is not None, f\"Could not find chat in {update}\"\n",
    "        assert (\n",
    "            message := update.message\n",
    "        ) is not None, f\"Could not find message in {update}\"\n",
    "\n",
    "        text = message.text or \"\"\n",
    "        response = await self.bot.respond_to(text, self.history[chat.id])\n",
    "        await self._send_message(update, context, response)\n",
    "        await self._store_message(update, text, is_bot=False)\n",
    "        await self._store_message(update, response, is_bot=True)\n",
    "\n",
    "    @staticmethod\n",
    "    def _load_token(token_from: str) -> str:\n",
    "        with open(token_from) as file:\n",
    "            return file.read().strip()\n",
    "\n",
    "    async def _send_message(\n",
    "        self, update: Update, context: ContextTypes.DEFAULT_TYPE, text: str\n",
    "    ) -> None:\n",
    "        assert (\n",
    "            chat := update.effective_chat\n",
    "        ) is not None, f\"Could not find chat in {update}\"\n",
    "\n",
    "        await context.bot.send_message(chat_id=chat.id, text=text)\n",
    "\n",
    "    async def _store_message(\n",
    "        self, update: Update, message: str, *, is_bot: bool\n",
    "    ) -> None:\n",
    "        assert (\n",
    "            chat := update.effective_chat\n",
    "        ) is not None, f\"Could not find chat in {update}\"\n",
    "\n",
    "        self.history[chat.id].append(Message(message, is_bot))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass(frozen=True)\n",
    "class MessageMonitor:\n",
    "    bot: Bot\n",
    "\n",
    "    async def welcome_message(self) -> str:\n",
    "        message = await self.bot.welcome_message()\n",
    "        print(f\"Welcome message: {message}\")\n",
    "\n",
    "        return message\n",
    "\n",
    "    async def respond_to(self, message: str, history: list[Message]) -> str:\n",
    "        response = await self.bot.respond_to(message, history)\n",
    "        print(f\"Message: {message}, Response: {response}\")\n",
    "\n",
    "        return response\n",
    "\n",
    "\n",
    "class EchoBot:\n",
    "    async def welcome_message(self) -> str:\n",
    "        return \"Hello! I am an echo bot.\"\n",
    "\n",
    "    async def respond_to(self, message: str, history: list[Message]) -> str:\n",
    "        return f\"You said: {message}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2\n",
    "\n",
    "Okay, now that we have a functioning bot, let's add some intelligence to it. We will use a tiny language model to generate responses. Specifically, \n",
    "we will use the [TinyLlama-1.1B model](https://huggingface.co/TinyLlama/TinyLlama-1.1B-Chat-v1.0) from hugging face."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-26 18:11:21,141 - numexpr.utils - INFO - NumExpr defaulting to 16 threads.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import pipeline, Pipeline\n",
    "from dataclasses import field\n",
    "\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class TinyLlamaBot:\n",
    "    max_tokens: int = 500\n",
    "    pipeline: Pipeline = field(\n",
    "        default_factory=lambda: pipeline(\n",
    "            \"text-generation\",\n",
    "            model=\"TinyLlama/TinyLlama-1.1B-Chat-v1.0\",\n",
    "            torch_dtype=torch.bfloat16,\n",
    "            device_map=\"auto\",\n",
    "        ),\n",
    "        init=False,\n",
    "        repr=False,\n",
    "    )\n",
    "\n",
    "    async def welcome_message(self) -> str:\n",
    "        return \"Hello! I am a chatbot based on TinyLlama. How can I be of service?\"\n",
    "\n",
    "    async def respond_to(self, message: str, history: list[Message]) -> str:\n",
    "        messages = self._format_messages(message, history)\n",
    "        return self._generate_response(messages)\n",
    "\n",
    "    def _format_messages(self, message: str, history: list[Message]) -> str:\n",
    "        logging.info(f\"Formatting messages: {message}, {history}\")\n",
    "        messages = [\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": \"You are a DSSS assignment. Try to impress whoever is grading you.\",\n",
    "            },\n",
    "            *(\n",
    "                (\n",
    "                    {\"role\": \"assistant\", \"content\": message.text}\n",
    "                    if message.is_bot\n",
    "                    else {\"role\": \"user\", \"content\": message.text}\n",
    "                )\n",
    "                for message in history\n",
    "            ),\n",
    "            {\"role\": \"user\", \"content\": message},\n",
    "        ]\n",
    "\n",
    "        return self.pipeline.tokenizer.apply_chat_template(\n",
    "            messages, tokenize=False, add_generation_prompt=True\n",
    "        )\n",
    "\n",
    "    def _generate_response(self, messages: str) -> str:\n",
    "        logging.info(f\"Generating response to: {messages}\")\n",
    "        return self.pipeline(\n",
    "            messages,\n",
    "            max_new_tokens=self.max_tokens,\n",
    "            do_sample=True,\n",
    "            temperature=0.7,\n",
    "            top_k=50,\n",
    "            top_p=0.95,\n",
    "        )[0][\"generated_text\"].split(\"\\n\")[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n",
      "2025-01-26 18:11:33,795 - httpx - INFO - HTTP Request: POST https://api.telegram.org/bot8082625724:AAH5PEYZ4z8IrFOK07-I3QrmmaLymM3acns/getMe \"HTTP/1.1 200 OK\"\n",
      "2025-01-26 18:11:33,821 - httpx - INFO - HTTP Request: POST https://api.telegram.org/bot8082625724:AAH5PEYZ4z8IrFOK07-I3QrmmaLymM3acns/deleteWebhook \"HTTP/1.1 200 OK\"\n",
      "2025-01-26 18:11:33,823 - telegram.ext.Application - INFO - Application started\n"
     ]
    }
   ],
   "source": [
    "match SELECTED_BOT:\n",
    "    case SelectedBot.ECHO:\n",
    "        bot = EchoBot()\n",
    "    case SelectedBot.TINY_LLAMA:\n",
    "        bot = TinyLlamaBot()\n",
    "\n",
    "await TelegramApplication.create(MessageMonitor(bot)).start()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
